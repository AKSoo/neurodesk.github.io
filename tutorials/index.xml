<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NeuroDesk – Tutorials</title><link>https://neurodesk.github.io/tutorials/</link><description>Recent content in Tutorials on NeuroDesk</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://neurodesk.github.io/tutorials/index.xml" rel="self" type="application/rss+xml"/><item><title>Tutorials: fMRI BIDS Conversion</title><link>https://neurodesk.github.io/tutorials/fmribids/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://neurodesk.github.io/tutorials/fmribids/</guid><description>
&lt;h2 id="download-demo-data">Download demo data&lt;/h2>
&lt;p>Here we would also like to show how to combine tools from different containers using the module system. Open a terminal and run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">ml datalad/0.13.3
cd /neurodesktop-storage/
datalad create bids-conversion
cd bids-conversion/
datalad clone --dataset . https://github.com/datalad/example-dicom-functional.git inputs/rawdata
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Tutorials: fMRIPrep</title><link>https://neurodesk.github.io/tutorials/fmriprep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://neurodesk.github.io/tutorials/fmriprep/</guid><description>
&lt;p>fMRIPrep is a functional magnetic resonance imaging (fMRI) data preprocessing pipeline that is designed to provide an easily accessible, state-of-the-art interface that is robust to variations in scan acquisition protocols and that requires minimal user input, while providing easily interpretable and comprehensive error and output reporting. It performs basic processing steps (coregistration, normalization, unwarping, noise component extraction, segmentation, skullstripping etc.) providing outputs that can be easily submitted to a variety of group level analyses, including task-based or resting-state fMRI, graph theory measures, surface or volume-based statistics, etc.&lt;/p>
&lt;h2 id="download-demo-data">Download demo data&lt;/h2>
&lt;p>We base this example on this blog-post: &lt;a href="https://reproducibility.stanford.edu/fmriprep-tutorial-running-the-docker-image/">https://reproducibility.stanford.edu/fmriprep-tutorial-running-the-docker-image/&lt;/a> and will also use the same data.&lt;/p>
&lt;p>Open a terminal and run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Tutorials: FreeSurfer</title><link>https://neurodesk.github.io/tutorials/freesurfer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://neurodesk.github.io/tutorials/freesurfer/</guid><description>
&lt;h2 id="download-demo-data">Download demo data&lt;/h2>
&lt;p>Open a terminal and run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">pip install osfclient
osf -p bt4ez fetch TOMCAT_DIB/sub-01/ses-01_7T/anat/sub-01_ses-01_7T_T1w_defaced.nii.gz /neurodesktop-storage/sub-01_ses-01_7T_T1w_defaced.nii.gz
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="freesurfer-license-file">FreeSurfer License file:&lt;/h2>
&lt;p>Before using Freesurfer you need to request a license here (&lt;a href="https://surfer.nmr.mgh.harvard.edu/registration.html">https://surfer.nmr.mgh.harvard.edu/registration.html&lt;/a>) and store it in your homedirectory as ~/.license&lt;/p>
&lt;h2 id="freesurfer-example">FreeSurfer Example&lt;/h2>
&lt;p>Open FreeSurfer (Neurodesk -&amp;gt; Image Segmentation -&amp;gt; Freesurfer -&amp;gt; Freesurfer 7.1.1)&lt;/p>
&lt;p>Setup FreeSurfer license (for example - replace with your license):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">echo &amp;#34;Steffen.Bollmann@cai.uq.edu.au
&amp;gt; 21029
&amp;gt; *Cqyn12sqTCxo
&amp;gt; FSxgcvGkNR59Y&amp;#34; &amp;gt;&amp;gt; ~/.license
export FS_LICENSE=~/.license
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Setup FreeSurfer:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">mkdir /neurodesktop-storage/freesurfer-output
source /opt/freesurfer-7.1.1/SetUpFreeSurfer.sh
export SUBJECTS_DIR=/neurodesktop-storage/freesurfer-output
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Run Recon all pipeline:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">recon-all -subject test-subject -i /neurodesktop-storage/sub-01_ses-01_7T_T1w_defaced.nii.gz -all
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Tutorials: Analysing EEG Data with MNE</title><link>https://neurodesk.github.io/tutorials/eeg_mne-python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://neurodesk.github.io/tutorials/eeg_mne-python/</guid><description>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>Open Visual Studio Code:&lt;/p>
&lt;p>&lt;img src="https://neurodesk.github.io/EEG_Tutorial/EEGtut1.png" alt="EEGtut1" title="EEGtut1">&lt;/p>
&lt;p>Open the folder: “/home/user/Desktop/neurodesktop-storage” or a subfolder in which you would like to store this demo. In this folder, create a new file named “EEGDemo.ipynb” or something similar:&lt;/p>
&lt;p>&lt;img src="https://neurodesk.github.io/EEG_Tutorial/EEGtut2.png" alt="EEGtut2" title="EEGtut2">&lt;/p>
&lt;p>If this is your first time opening a Jupyter notebook on vscode in neurodesktop, you will see the following popup. If so, click “install” to install the vscode extensions for Jupyter.&lt;/p>
&lt;p>&lt;img src="https://neurodesk.github.io/EEG_Tutorial/EEGtut3.png" alt="EEGtut3" title="EEGtut3">&lt;/p>
&lt;h2 id="set-up-an-environment">Set up an environment&lt;/h2>
&lt;blockquote>
&lt;p>Coming soon! Neurodesktop will soon feature a number of built in conda environments for standard analyses of behavioural, physiological, and encephalographic data.&lt;/p>
&lt;/blockquote>
&lt;p>From the top menu in vscode, select Terminal-&amp;gt;New Terminal, or hit [Ctrl]+[Shift]+[`]. From this terminal, create and activate a new conda environment in which to run mne-python.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">conda create --name=mne --channel=conda-forge mne python=3 jupyter nb_conda_kernels
conda activate mne
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once your new environment is activated, in the top right corner of your empty jupyter notebook, click “Select Kernel”:&lt;/p>
&lt;p>&lt;img src="https://neurodesk.github.io/EEG_Tutorial/EEGtut4.png" alt="EEGtut4" title="EEGtut4">&lt;/p>
&lt;p>Then, select the instance of Python associated with the environment you have just created (“mne”):&lt;/p>
&lt;p>&lt;img src="https://neurodesk.github.io/EEG_Tutorial/EEGtut5.png" alt="EEGtut5" title="EEGtut5">&lt;/p>
&lt;p>At this point you may also be prompted to install the vscode packages for python. Once you have installed these, you’re ready to rumble!&lt;/p>
&lt;h2 id="download-sample-data">Download sample data&lt;/h2>
&lt;p>In the terminal, input the following code to download some BIDS formatted sample EEG data:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">pip install osfclient
osf -p C689U fetch Data_sample.zip /neurodesktop-storage/EEGDEMO/Data_sample.zip
unzip Data_sample.zip
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is a small dataset with only 5 EEG channels from a single participant. The participant is viewing a frequency tagged display and is cued to attend to dots tagged at one frequency or another (6 Hz, 7.5 Hz) for long, 15 s trials. To read more about the dataset, click &lt;a href="https://osf.io/c689u/">here&lt;/a>&lt;/p>
&lt;h2 id="plotting-settings">Plotting settings&lt;/h2>
&lt;p>To make sure our plots retain their interactivity, set the following line at the top of your notebook:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">%matplotlib qt
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will mean your figures pop out as individual, interactive plots that will allow you to explore the data, rather than as static, inline plots. You can switch “qt” to “inline” to switch back to default, inline plotting.&lt;/p>
&lt;h2 id="loading-and-processing-data">Loading and processing data&lt;/h2>
&lt;blockquote>
&lt;p>NOTE: MNE has many helpful tutorials which delve into data processing and analysis using MNE-python in much further detail. These can be found &lt;a href="https://mne.tools/stable/auto_tutorials/index.html">here&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Begin by importing the necessary modules and creating a pointer to the data:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback"># Interactive plotting
%matplotlib qt
# Import modules
import os
import numpy as np
import mne
# Load data
sample_data_folder = &amp;#39;/home/user/Desktop/neurodesktop-storage/EEGDemo/Data_sample&amp;#39;
sample_data_raw_file = os.path.join(sample_data_folder, &amp;#39;sub-01&amp;#39;, &amp;#39;eeg&amp;#39;,
&amp;#39;sub-01_task-FeatAttnDec_eeg.vhdr&amp;#39;)
raw = mne.io.read_raw_brainvision(sample_data_raw_file , preload=True)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>the raw.info structure contains information about the dataset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback"># Display data info
print(raw)
print(raw.info)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This data file did not include a montage. Lets create one using standard values for the electrodes we have:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback"># Create montage
montage = {&amp;#39;Iz&amp;#39;: [0, -110, -40],
&amp;#39;Oz&amp;#39;: [0, -105, -15],
&amp;#39;POz&amp;#39;: [0, -100, 15],
&amp;#39;O1&amp;#39;: [-40, -106, -15],
&amp;#39;O2&amp;#39;: [40, -106, -15],
}
montageuse = mne.channels.make_dig_montage(ch_pos=montage, lpa=[-82.5, -19.2, -46], nasion=[0, 83.2, -38.3], rpa=[82.2, -19.2, -46]) # based on mne help file on setting 10-20 montage
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, lets visualise the data.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">raw.plot()
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This should open an interactive window in which you can scroll through the data. See the MNE documentation for help on how to customise this plot.&lt;/p>
&lt;p>&lt;img src="https://neurodesk.github.io/EEG_Tutorial/EEGtut6.png" alt="EEGtut6" title="EEGtut6">&lt;/p>
&lt;p>If, upon visual inspection, you decide to exclude one of the channels, you can specify this in raw.info[‘bads’] now. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">raw.info[&amp;#39;bads&amp;#39;] = [&amp;#39;POz&amp;#39;]
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, we’ll extract our events. The trigger channel in this file is incorrectly scaled, so we’ll correct that before we extract our events:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback"># Correct trigger scaling
trigchan = raw.pick(&amp;#39;TRIG&amp;#39;).copy()
trigchan._data = trigchan._data*1000000
# Extract events
events = mne.find_events(trigchan, stim_channel=&amp;#39;TRIG&amp;#39;, consecutive=True, initial_event=True, verbose=True)
print(&amp;#39;Found %s events, first five:&amp;#39; % len(events))
print(events[:5])
# Plot events
mne.viz.plot_events(events, raw.info[&amp;#39;sfreq&amp;#39;], raw.first_samp)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://neurodesk.github.io/EEG_Tutorial/EEGtut7.png" alt="EEGtut7" title="EEGtut7">&lt;/p>
&lt;p>Now that we’ve extracted our events, we can extract our EEG channels and do some simple pre-processing:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback"># select
eeg_data = raw.copy().pick_types(eeg=True, exclude=[&amp;#39;TRIG&amp;#39;])
# Set montage
eeg_data.info.set_montage(montageuse)
# Interpolate
eeg_data_interp = eeg_data.copy().interpolate_bads(reset_bads=True)
# Filter Data
eeg_data_interp.filter(l_freq=1, h_freq=45, h_trans_bandwidth=0.1)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let’s visualise our data again now that it’s cleaner:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">#plot results again, this time with some events and scaling.
eeg_data_interp.plot(events=events, duration=10.0, scalings=dict(eeg=0.00005), color=&amp;#39;k&amp;#39;, event_color=&amp;#39;r&amp;#39;)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://neurodesk.github.io/EEG_Tutorial/EEGtut8.png" alt="EEGtut8" title="EEGtut8">&lt;/p>
&lt;p>That’s looking good! We can even see hints of the frequency tagging. It’s about time to epoch our data.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback"># Epoch to events of interest
event_id = {&amp;#39;attend 6Hz K&amp;#39;: 23, &amp;#39;attend 7.5Hz K&amp;#39;: 27}
# Extract 15 s epochs relative to events, baseline correct, linear detrend, and reject
# epochs where eeg amplitude is &amp;gt; 400
epochs = mne.Epochs(eeg_data_interp, events, event_id=event_id, tmin=0,
tmax=15, baseline=(0, 0), reject=dict(eeg=0.000400), detrend=1)
# Drop bad trials
epochs.drop_bad()
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can average these epochs to form Event Related Potentials (ERPs):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback"># Average erpochs to form ERPs
attend6 = epochs[&amp;#39;attend 6Hz K&amp;#39;].average()
attend75 = epochs[&amp;#39;attend 7.5Hz K&amp;#39;].average()
# Plot ERPs
evokeds = dict(attend6=list(epochs[&amp;#39;attend 6Hz K&amp;#39;].iter_evoked()),
attend75=list(epochs[&amp;#39;attend 7.5Hz K&amp;#39;].iter_evoked()))
mne.viz.plot_compare_evokeds(evokeds, combine=&amp;#39;mean&amp;#39;)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://neurodesk.github.io/EEG_Tutorial/EEGtut9.png" alt="EEGtut9" title="EEGtut9">&lt;/p>
&lt;p>From this point, we can export our data to numpy arrays or keep it in the MNE data format for any further analyses we would like to run.&lt;/p>
&lt;p>Congratulations! You’ve run your first analysis of EEG data in neurodesktop.&lt;/p></description></item><item><title>Tutorials: MRIQC</title><link>https://neurodesk.github.io/tutorials/mriqc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://neurodesk.github.io/tutorials/mriqc/</guid><description>
&lt;h2 id="download-demo-data">Download demo data&lt;/h2>
&lt;p>Open a terminal and run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Tutorials: Quantitative Susceptibility Mapping</title><link>https://neurodesk.github.io/tutorials/qsm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://neurodesk.github.io/tutorials/qsm/</guid><description>
&lt;h2 id="quantitative-susceptibility-mapping-in-qsmxt">Quantitative Susceptibility Mapping in QSMxT&lt;/h2>
&lt;p>Neurodesk includes QSMxT, a complete and end-to-end QSM processing and analysis framework that excels at automatically reconstructing and processing QSM for large groups of participants.&lt;/p>
&lt;p>QSMxT provides pipelines implemented in Python that:&lt;/p>
&lt;ol>
&lt;li>Automatically convert DICOM data to the Brain Imaging Data Structure (BIDS)&lt;/li>
&lt;li>Automatically reconstruct QSM, including steps for:
&lt;ol>
&lt;li>Robust masking without anatomical priors&lt;/li>
&lt;li>Phase unwrapping (Laplacian based)&lt;/li>
&lt;li>Background field removal + dipole inversion (&lt;code>tgv_qsm&lt;/code>)&lt;/li>
&lt;li>Multi-echo combination&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Automatically generate a common group space for the whole study, as well as average magnitude and QSM images that facilitate group-level analyses.&lt;/li>
&lt;li>Automatically segment T1w data and register them to the QSM space to extract quantitative values in anatomical regions of interest.&lt;/li>
&lt;li>Export quantitative data to CSV for all subjects using the automated segmentations, or a custom segmentation in the group space (we recommend ITK snap).&lt;/li>
&lt;/ol>
&lt;p>If you use QSMxT for a study, please cite &lt;a href="https://doi.org/10.1101/2021.05.05.442850">https://doi.org/10.1101/2021.05.05.442850&lt;/a>.&lt;/p>
&lt;h2 id="download-demo-data">Download demo data&lt;/h2>
&lt;p>Open a terminal and run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">pip install osfclient
cd /neurodesktop-storage/
osf -p ru43c clone /neurodesktop-storage/qsmxt-demo
unzip /neurodesktop-storage/qsmxt-demo/osfstorage/GRE_2subj_1mm_TE20ms/sub1/GR_M_5_QSM_p2_1mmIso_TE20.zip -d /neurodesktop-storage/qsmxt-demo/dicoms
unzip /neurodesktop-storage/qsmxt-demo/osfstorage/GRE_2subj_1mm_TE20ms/sub1/GR_P_6_QSM_p2_1mmIso_TE20.zip -d /neurodesktop-storage/qsmxt-demo/dicoms
unzip /neurodesktop-storage/qsmxt-demo/osfstorage/GRE_2subj_1mm_TE20ms/sub2/GR_M_5_QSM_p2_1mmIso_TE20.zip -d /neurodesktop-storage/qsmxt-demo/dicoms
unzip /neurodesktop-storage/qsmxt-demo/osfstorage/GRE_2subj_1mm_TE20ms/sub2/GR_P_6_QSM_p2_1mmIso_TE20.zip -d /neurodesktop-storage/qsmxt-demo/dicoms
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="qsmxt-usage">QSMxT Usage&lt;/h2>
&lt;p>Start QSMxT (in this demo we used 1.1.6) from the applications menu in the desktop (&lt;em>Neurodesk&lt;/em> &amp;gt; &lt;em>Quantitative Imaging&lt;/em> &amp;gt; &lt;em>qsmxt&lt;/em>)&lt;/p>
&lt;ol>
&lt;li>Convert DICOM data to BIDS:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#204a87">cd&lt;/span> /neurodesktop-storage/qsmxt-demo
python3 /opt/QSMxT/run_0_dicomSort.py /neurodesktop-storage/qsmxt-demo/dicoms 00_dicom
python3 /opt/QSMxT/run_1_dicomToBids.py 00_dicom 01_bids
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;p>After this step check if the data were correctly recognized and converted to BIDS. Otherwise make a copy of /opt/QSMxT/bidsmap.yaml - adjust based on provenance example in 01_bids/code/bidscoin/bidsmap.yaml (see for example what it detected under extra_files) - and run again with the parameter &lt;code>--heuristic bidsmap.yaml&lt;/code>. If the data were acquired on a GE scanner the complex data needs to be corrected by applying an FFT shift, this can be done with &lt;code>python /opt/QSMxT/run_1_fixGEphaseFFTshift.py 01_bids/sub*/ses*/anat/*_run-1_*.nii.gz&lt;/code> .&lt;/p>
&lt;ol start="2">
&lt;li>Run QSM pipeline:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">python3 /opt/QSMxT/run_2_qsm.py 01_bids 02_qsm_output
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol></description></item></channel></rss>